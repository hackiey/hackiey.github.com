<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="To be a hacker">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>人工神经网络（线性单元）（梯度下降） - Harry的博客 | Harry's Blog</title>

    <link rel="canonical" href="http://words.hackiey.com/2016/05/15/ann-perceptron-2/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="http://cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Hackiey</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/in-post/ann/ann2.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/in-post/ann/ann2.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
                        
                        <a class="tag" href="/tags/#人工神经网络" title="人工神经网络">人工神经网络</a>
                        
                    </div>
                    <h1>人工神经网络（线性单元）（梯度下降）</h1>
                    
                    
                    <h2 class="subheading"></h2>
                    
                    <span class="meta">Posted by Harry on May 15, 2016</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<p>感知器训练法则所能解决的问题是非常有限的，大部分的问题都是线性不可分的，所以为了保证在线性不可分时仍能够使训练过程收敛，要引入delta法则。它同时也是学习多个单元的互联网络的基础。</p>

<h3 id="delta">delta法则</h3>

<p>delta法则的关键思想是使用梯度下降(gradient descent)搜索“所有“（由于程序在训练时不可能完全平滑，所以设置一个足够小的<script type="math/tex">\eta</script>来尽可能的在误差最小值附近逼近这个值）可能的权值假设空间，从中找出能够最佳拟合训练样例的权向量。</p>

<blockquote>
  <p>最好把delta训练法则理解为训练一个无阈值的感知器，也就是一个线性单元(linear unit)，它的输出o如下:</p>
</blockquote>

<script type="math/tex; mode=display">o(\vec x) = \vec w \cdot \vec x \tag{1}</script>

<p>为了理解梯度下降，首先先定义训练误差:</p>

<script type="math/tex; mode=display">E(\vec w)={1\over 2}\sum_{d\in D}(t_d-o_d)^2 \tag{2}</script>

<p>这是最常用的用来度量误差的定义，其中D是训练样例集合，<script type="math/tex">t_d</script>是训练样例d的目标输出，<script type="math/tex">o_d</script>是线性单元对训练样例d的输出，<code class="highlighter-rouge">形式上如同方差，至于为什么要乘1/2，可能是为了推导出最后的公式后有一个简洁的运算。</code>一个有两个输入的线性单元对应的误差平面图如下，从这里可以直观的看出，当随机初始化权值后，误差有可能在误差平面上任何一个点，为了到达<script type="math/tex">E(\vec w)</script>的最小值，要不断去寻找当前点在这个平面上<strong>最陡</strong>的方向并朝着这个方向进行移动。这里可以看出为什么delta法则总能收敛。<code class="highlighter-rouge">如果训练数据不能线性可分，那么这个误差平面与</code><script type="math/tex">w_1</script><code class="highlighter-rouge">和</code><script type="math/tex">w_2</script><code class="highlighter-rouge">组成的平面不会相交，即使收敛也无法学习到最正确的目标函数(考虑异或函数，你无论如和也无法找出两个权值来拟合这个目标函数)。</code></p>

<p><img src="/img/in-post/ann/perceptron/error.png" alt="不同假设的误差" /></p>

<h3 id="section">梯度下降法则的推导</h3>

<p>推导过程的关键是找出沿误差曲面最陡峭下降的方向。可以通过计算E相对向量<script type="math/tex">\vec w</script>的<strong>每个分量的导数</strong>来得到这个<strong>方向</strong>。这个向量导数被称为E对于<script type="math/tex">\vec w</script>的梯度(gradient)，记作<script type="math/tex">\nabla E(\vec w)</script>。</p>

<script type="math/tex; mode=display">\nabla E(\vec w)\equiv \left [
		{\partial E\over \partial w_0} ,
		{\partial E\over \partial w_1} ,
		... ,
		{\partial E\over \partial w_n} ,
	\right ]\tag{3}</script>

<p>梯度本身是一个向量，没有具体的距离量度，因此梯度下降的训练法则是:</p>

<script type="math/tex; mode=display">\vec w\leftarrow \vec w+ \Delta \vec w</script>

<p>其中:</p>

<script type="math/tex; mode=display">\Delta \vec w=- \eta \nabla E(\vec w) \tag {4}</script>

<p><script type="math/tex">\eta</script>是学习速率，它决定了在梯度下降中向误差最小值移动的速度。为了获取最终的权值更新公式(<script type="math/tex">w_i</script>的更新公式)，可以写出其分量形式:</p>

<script type="math/tex; mode=display">w_i\leftarrow w_i +\Delta w_i</script>

<p>其中:</p>

<script type="math/tex; mode=display">\Delta w_i=-\eta {\partial E\over\partial w_i} \tag{5}</script>

<p>现在来推导一下<script type="math/tex">\partial E \over \partial w_i</script>,根据(2)式可以得出第一步，然后根据求导法则逐步推导，需要注意的是，<script type="math/tex">t_d</script><code class="highlighter-rouge">是训练数据的目标输出，它与</code><script type="math/tex">w_i</script><code class="highlighter-rouge">无关，而</code><script type="math/tex">o_d</script><code class="highlighter-rouge">包含了在公式中充当未知数的</code><script type="math/tex">w_i</script><code class="highlighter-rouge">，因此最后一步求导时</code><script type="math/tex">t_d</script><code class="highlighter-rouge">被视为一个常数最终成为0。这个推导过程十分重要，它是梯度下降算法的核心过程。</code></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
{\partial E\over \partial w_i} & = {\partial \over \partial w_i}{1\over 2}\sum_{d\in D}(t_d-o_d)^2 = {1\over 2}\sum_{d\in D}{\partial \over \partial w_i}(t_d-o_d)^2\\
& = {1\over 2}\sum_{d\in D}2(t_d-o_d){\partial \over \partial w_i}(t_d-o_d) \\
& = \sum_{d\in D}(t_d-o_d){\partial \over \partial w_i}(t_d-\vec w \cdot \vec x_d) \\
{\partial E\over \partial w_i} & = \sum_{d\in D}(t_d-o_d)(-x_{id}) \tag{6}\\
\end{align} %]]></script>

<p>其中<script type="math/tex">x_{id}</script>是训练样例d的一个输入分量<script type="math/tex">x_i</script>。将(6)代入(5)中可以得出最终的梯度下降权值更新法则:</p>

<script type="math/tex; mode=display">\nabla w_i=\eta \sum_{d\in D}(t_d-o_d)x_{id} \tag{7}</script>

<h3 id="section-1">训练线性单元的梯度下降算法</h3>

<hr />
<p><strong>GRADIENT-DESCENT(training_examples, <script type="math/tex">\eta</script>)</strong></p>

<p>training_examples中每一个训练样例形式为序偶&lt;<script type="math/tex">\vec x</script>, t&gt;，其中<script type="math/tex">\vec x</script>是输入值向量，t是目标输出值，<script type="math/tex">\eta</script>是学习速率(例如0.05)</p>

<ul>
  <li>初始化每个<script type="math/tex">w_i</script>为某个小的随机值</li>
  <li>遇到终止条件之前，做以下操作:
    <ul>
      <li>初始化每个<script type="math/tex">\nabla w_i</script>为0</li>
      <li>对于训练样例training_examples中的每个&lt;<script type="math/tex">\vec x</script>, t&gt;，做:
        <ul>
          <li>把实例<script type="math/tex">\vec x</script>输入到此单元，计算出o</li>
          <li>对于线性单元的每个权<script type="math/tex">w_i</script>，做:</li>
        </ul>

        <script type="math/tex; mode=display">\Delta w_i\leftarrow \Delta w_i +\eta (t-o)x_i \tag8</script>
      </li>
      <li>
        <p>对于线性单元的每个权<script type="math/tex">w_i</script>，做:</p>

        <script type="math/tex; mode=display">w_i \leftarrow w_i+\Delta w_i \tag9</script>
      </li>
    </ul>
  </li>
</ul>

<hr />
<p>你可能会发现算法中没有出现<script type="math/tex">t_d或o_d</script>之类的符号，在对训练样例的循环中，t和o就代表<script type="math/tex">t_d和o_d</script>，一次training_examples的循环结束之后，使用不断累加得到的<script type="math/tex">\Delta w_i</script>对<script type="math/tex">w_i</script>更新。从算法的描述中就可以看出，每一次权值更新都要应用所有的训练样例到线性单元并计算出<script type="math/tex">\Delta w_i</script>，因此训练过程是极其缓慢的。<code class="highlighter-rouge">算法中所提到的终止条件指的是事先设定一个可以接受的误差范围，当误差小于这个范围时则终止。</code></p>

<h3 id="section-2">实验</h3>

<p>这一节实验使用书中的一个练习题：</p>

<blockquote>
  <p>实现一个两输入线性单元的delta训练法则。训练它来拟合目标概念 <script type="math/tex">-2+x_1+2x_2>0</script>。画出误差E相对训练迭代次数的函数曲线。画出5，10，50，100，……次迭代后的决策面。
(a)为 <script type="math/tex">\eta</script> 选取不同的常量值，并使用衰减的学习速率——也就是第 i 次迭代使用 <script type="math/tex">\eta_0 / i</script>，再进行实验。哪一个效果更好？</p>
</blockquote>

<p>在我动手去做这个实验之前，我有两个疑问：
（1）为什么一个线性单元学习的目标概念有不等式？（2）它的训练样例是怎样的？</p>

<p>翻了一遍书之后，我认为答案是这样的:<code class="highlighter-rouge">（1）不等式代表了一种线性划分，它是二维平面的分类器。如果没有不等式，它就是一条直线，这条直线正是我们要学习的决策面，而决策面只有在不等式的条件下才有存在的意义（没有不等式的划分，哪里会有决策面呢）。(2)在感知器单元中，它的输出就像一个分类器，而在这里我们所应用的是一个回归问题。针对这个目标函数，它的训练样例输出就是 </code><script type="math/tex">-2+x_1+2x_2</script><code class="highlighter-rouge">，这样最终学习到的权值</code><script type="math/tex">\vec w</script><code class="highlighter-rouge">与</code><script type="math/tex">\vec x</script><code class="highlighter-rouge">进行点乘就是最后的决策面(</code><script type="math/tex">\vec w\cdot \vec x=0</script><code class="highlighter-rouge">)。</code></p>

<p>做这个实验的过程中，我体会到了理论研究中很少注意到的一个参数——学习速率，其实在感知器中就已经说明了学习过程的收敛条件之一是<script type="math/tex">\eta</script>要足够小，如果<script type="math/tex">\eta</script>没有足够小，那么权值会不断增大到无穷大。一个基本的技巧是如果你的训练数据中输入的数字都比较大，那么学习速率一定要缩小。例如你的训练数据范围在[-0.1,0.1]，那么你可能需要一个<script type="math/tex">10^{-6}</script>数量级的<script type="math/tex">\eta</script>，如果你的训练数据范围在[-1,1]，那么你需要更小的<script type="math/tex">\eta</script>，你可以监控学习过程中所得到的权值来不断调整这一参数。</p>

<p>本实验的训练数据使用了[-0.1,0.1]的输入范围，学习速率为0.000005，在E(梯度下降所设定的误差)小于0.1时结束循环。为了快速收敛，我偷了一些小懒，一是直接将<script type="math/tex">w_0</script>设为0，<script type="math/tex">w_1和w_2</script>设置为大于0的小的随机数（正常的设置权值应该是[-x,x]中的随机值，x具体应该取决于你所学习的对象）；二是没有采用a)问中的<script type="math/tex">\eta_0 / i</script>学习速率(使用这个参数之后，在接近收敛时仍会花费很多时间，但是这是一个好的策略，它会尽可能的避免你跃过最小值点)。</p>

<p>学习过程总共经过了136轮权值修正。下图中是前30轮的权值和误差E(可以看出E从开始的急速下降到后面的缓慢下降)。最终的权值为[-1.996601 1.003349 2.002383]。</p>

<p><img src="/img/in-post/ann/perceptron/linear-unit-experiment.png" alt="实验前30轮权值及误差E" /></p>

<p>误差E的前30轮曲线图：</p>

<p><img src="/img/in-post/ann/perceptron/errorE.png" alt="误差E曲线图" /></p>

<h3 id="section-3">随机梯度下降</h3>

<p>在上面的误差曲面图中只有一个最小值，实际上在很多实际应用中有多个局部最小值，那么就不能保证最终会收敛到全局最小值。缓解这个困难的一个常见的梯度下降变体被称为增量梯度下降(incremental gradient descent)或随机梯度下降(stochastic gradient descent)。随机梯度下降可被看作为每个单独的训练样例d定义不同的误差函数<script type="math/tex">E_d(\vec w)</script>：</p>

<script type="math/tex; mode=display">E_d(\vec w)= {1\over 2} (t_d-o_d)^2 \tag{10}</script>

<p>根据误差公式所计算的<script type="math/tex">\Delta w_i</script>为：</p>

<script type="math/tex; mode=display">\Delta w_i=\eta (t-o)x_i \tag{11}</script>

<p>将(8)替换为(11)并删掉(9)就是随机梯度下降算法。标准的梯度下降和随机的梯度下降之间的关键区别是：</p>

<ul>
  <li>标准的梯度下降是权值更新前对所有样例汇总误差，而随机梯度下降的权值是通过考察每个训练实例来更新的。</li>
  <li>在标准的梯度下降中，权值更新的每一步对多个样例求和，这需要更多的计算。另一方面，因为使用真正的梯度，标准的梯度下降对于每一次权值更新经常使用比随机梯度下降大的步长。</li>
  <li>如果<script type="math/tex">E(\vec w)</script>有多个局部极小值，随机的梯度下降有时可能避免陷入这些局部极小值中，因为它使用不同的<script type="math/tex">\nabla E_d(\vec w)</script>而不是<script type="math/tex">\nabla E(\vec w)</script>来引导搜索。</li>
</ul>


                <hr>

                


                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2016/05/14/ann-perceptron-1/" data-toggle="tooltip" data-placement="top" title="人工神经网络（感知器）">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2016/06/21/backpropagation/" data-toggle="tooltip" data-placement="top" title="人工神经网络（多层网络和BP算法）（一）">Next Post &rarr;</a>
                    </li>
                    
                </ul>


                

                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

            </div>

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				
                            
        				
                            
                				<a href="/tags/#机器学习" title="机器学习" rel="6">
                                    机器学习
                                </a>
                            
        				
                            
                				<a href="/tags/#人工神经网络" title="人工神经网络" rel="4">
                                    人工神经网络
                                </a>
                            
        				
        			</div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="http://www.hackiey.com">Hackiey</a></li>
                    
                        <li><a href="http://gty.org.in/">David Gu</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>





<!-- disqus 公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "hackiey";
    var disqus_identifier = "/2016/05/15/ann-perceptron-2";
    var disqus_url = "http://words.hackiey.com/2016/05/15/ann-perceptron-2/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus 公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("http://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    


                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/hackiey">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Hackiey 2016
                    <br>
<!--                     Theme by <a href="http://huangxuan.me">Hux</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true" >
                    </iframe> -->
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("http://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->




<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->
<!-- script -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
