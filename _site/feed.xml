<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hackiey</title>
    <description>To be a hacker</description>
    <link>http://words.hackiey.com/</link>
    <atom:link href="http://words.hackiey.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 18 May 2016 02:42:49 +0800</pubDate>
    <lastBuildDate>Wed, 18 May 2016 02:42:49 +0800</lastBuildDate>
    <generator>Jekyll v3.1.3</generator>
    
      <item>
        <title>人工神经网络（线性单元）（delta法则）</title>
        <description>&lt;p&gt;感知器训练法则所能解决的问题是非常有限的，大部分的问题都是线性不可分的，所以为了保证在线性不可分时仍能够使训练过程收敛，要引入delta法则。它同时也是学习多个单元的互联网络的基础。&lt;/p&gt;

&lt;h3 id=&quot;delta&quot;&gt;delta法则&lt;/h3&gt;

&lt;p&gt;delta法则的关键思想是使用梯度下降(gradient descent)搜索“所有“（由于程序在训练时不可能完全平滑，所以设置一个足够小的&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;来尽可能的在误差最小值附近逼近这个值）可能的权值假设空间，从中找出能够最佳拟合训练样例的权向量。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;最好把delta训练法则理解为训练一个无阈值的感知器，也就是一个线性单元(linear unit)，它的输出o如下:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o(\vec x) = \vec w \cdot \vec x \tag{1}&lt;/script&gt;

&lt;p&gt;为了理解梯度下降，首先先定义训练误差:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(\vec w)={1\over 2}\sum_{d\epsilon D}(t_d-o_d)^2 \tag{2}&lt;/script&gt;

&lt;p&gt;这是最常用的用来度量误差的定义，其中D是训练样例集合，&lt;script type=&quot;math/tex&quot;&gt;t_d&lt;/script&gt;是训练样例d的目标输出，&lt;script type=&quot;math/tex&quot;&gt;o_d&lt;/script&gt;是线性单元对训练样例d的输出，&lt;code class=&quot;highlighter-rouge&quot;&gt;形式上如同方差，至于为什么要乘1/2，可能是为了推导出最后的公式后有一个简洁的运算。&lt;/code&gt;一个有两个输入的线性单元对应的误差平面图如下，从这里可以直观的看出，当随机初始化权值后，误差有可能在误差平面上任何一个点，为了到达&lt;script type=&quot;math/tex&quot;&gt;E(\vec w)&lt;/script&gt;的最小值，要不断去寻找当前点在这个平面上&lt;strong&gt;最陡&lt;/strong&gt;的方向并朝着这个方向进行移动。这里可以看出为什么delta法则总能收敛。&lt;code class=&quot;highlighter-rouge&quot;&gt;如果训练数据不能线性可分，那么这个误差平面与&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;和&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;组成的平面不会相交，即使收敛也无法学习到最正确的目标函数(考虑异或函数，你无论如和也无法找出两个权值来拟合这个目标函数)。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/ann/perceptron/error.png&quot; alt=&quot;不同假设的误差&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;梯度下降法则的推导&lt;/h3&gt;

&lt;p&gt;推导过程的关键是找出沿误差曲面最陡峭下降的方向。可以通过计算E相对向量&lt;script type=&quot;math/tex&quot;&gt;\vec w&lt;/script&gt;的&lt;strong&gt;每个分量的导数&lt;/strong&gt;来得到这个&lt;strong&gt;方向&lt;/strong&gt;。这个向量导数被称为E对于&lt;script type=&quot;math/tex&quot;&gt;\vec w&lt;/script&gt;的梯度(gradient)，记作&lt;script type=&quot;math/tex&quot;&gt;\nabla E(\vec w)&lt;/script&gt;。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla E(\vec w)\equiv \left [
		{\partial E\over \partial w_0} ,
		{\partial E\over \partial w_1} ,
		... ,
		{\partial E\over \partial w_n} ,
	\right ]\tag{3}&lt;/script&gt;

&lt;p&gt;梯度本身是一个向量，没有具体的距离量度，因此梯度下降的训练法则是:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec w\leftarrow \vec w+ \Delta \vec w&lt;/script&gt;

&lt;p&gt;其中:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta \vec w=- \eta \nabla E(\vec w) \tag {4}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;是学习速率，它决定了在梯度下降中向误差最小值移动的速度。为了获取最终的权值更新公式(&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;的更新公式)，可以写出其分量形式:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i\leftarrow w_i +\Delta w_i&lt;/script&gt;

&lt;p&gt;其中:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w_i=-\eta {\partial E\over\partial w_i} \tag{5}&lt;/script&gt;

&lt;p&gt;现在来推导一下&lt;script type=&quot;math/tex&quot;&gt;\partial E \over \partial w_i&lt;/script&gt;,根据(2)式可以得出第一步，然后根据求导法则逐步推导，需要注意的是，&lt;script type=&quot;math/tex&quot;&gt;t_d&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;是训练数据的目标输出，它与&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;无关，而&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;o_d&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;包含了在公式中充当未知数的&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;，因此最后一步求导时&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;t_d&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;被视为一个常数最终成为0。这个推导过程十分重要，它是梯度下降算法的核心过程。&lt;/code&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
{\partial E\over \partial w_i} &amp; = {\partial \over \partial w_i}{1\over 2}\sum_{d\epsilon D}(t_d-o_d)^2 = {1\over 2}\sum_{d\epsilon D}{\partial \over \partial w_i}(t_d-o_d)^2\\
&amp; = {1\over 2}\sum_{d\epsilon D}2(t_d-o_d){\partial \over \partial w_i}(t_d-o_d) \\
&amp; = \sum_{d\epsilon D}(t_d-o_d){\partial \over \partial w_i}(t_d-\vec w \cdot \vec x_d) \\
{\partial E\over \partial w_i} &amp; = \sum_{d\epsilon D}(t_d-o_d)(-x_{id}) \tag{6}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;x_{id}&lt;/script&gt;是训练样例d的一个输入分量&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;。将(6)代入(5)中可以得出最终的梯度下降权值更新法则:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla w_i=\eta \sum_{d\epsilon D}(t_d-o_d)x_{id} \tag{7}&lt;/script&gt;

&lt;h3 id=&quot;section-1&quot;&gt;训练线性单元的梯度下降算法&lt;/h3&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;GRADIENT-DESCENT(training_examples, &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;training_examples中每一个训练样例形式为序偶&amp;lt;&lt;script type=&quot;math/tex&quot;&gt;\vec x&lt;/script&gt;, t&amp;gt;，其中&lt;script type=&quot;math/tex&quot;&gt;\vec x&lt;/script&gt;是输入值向量，t是目标输出值，&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;是学习速率(例如0.05)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;初始化每个&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;为某个小的随机值&lt;/li&gt;
  &lt;li&gt;遇到终止条件之前，做以下操作:
    &lt;ul&gt;
      &lt;li&gt;初始化每个&lt;script type=&quot;math/tex&quot;&gt;\nabla w_i&lt;/script&gt;为0&lt;/li&gt;
      &lt;li&gt;对于训练样例training_examples中的每个&amp;lt;&lt;script type=&quot;math/tex&quot;&gt;\vec x&lt;/script&gt;, t&amp;gt;，做:
        &lt;ul&gt;
          &lt;li&gt;把实例&lt;script type=&quot;math/tex&quot;&gt;\vec x&lt;/script&gt;输入到此单元，计算出o&lt;/li&gt;
          &lt;li&gt;对于线性单元的每个权&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;，做:&lt;/li&gt;
        &lt;/ul&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w_i\leftarrow \Delta w_i +\eta (t-o)x_i \tag8&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;对于线性单元的每个权&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;，做:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i \leftarrow w_i+\Delta w_i \tag9&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;你可能会发现算法中没有出现&lt;script type=&quot;math/tex&quot;&gt;t_d或o_d&lt;/script&gt;之类的符号，在对训练样例的循环中，t和o就代表&lt;script type=&quot;math/tex&quot;&gt;t_d和o_d&lt;/script&gt;，一次training_examples的循环结束之后，使用不断累加得到的&lt;script type=&quot;math/tex&quot;&gt;\Delta w_i&lt;/script&gt;对&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;更新。从算法的描述中就可以看出，每一次权值更新都要应用所有的训练样例到线性单元并计算出&lt;script type=&quot;math/tex&quot;&gt;\Delta w_i&lt;/script&gt;，因此训练过程是极其缓慢的。&lt;code class=&quot;highlighter-rouge&quot;&gt;算法中所提到的终止条件指的是事先设定一个可以接受的误差范围，当误差小于这个范围时则终止。&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;实验&lt;/h3&gt;

&lt;p&gt;这一节实验使用书中的一个练习题：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;实现一个两输入线性单元的delta训练法则。训练它来拟合目标概念 &lt;script type=&quot;math/tex&quot;&gt;-2+x_1+2x_2&gt;0&lt;/script&gt;。画出误差E相对训练迭代次数的函数曲线。画出5，10，50，100，……次迭代后的决策面。
(a)为 &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; 选取不同的常量值，并使用衰减的学习速率——也就是第 i 次迭代使用 &lt;script type=&quot;math/tex&quot;&gt;\eta_0 / i&lt;/script&gt;，再进行实验。哪一个效果更好？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在我动手去做这个实验之前，我有很多疑问：
（1）为什么一个线性单元学习的目标概念有不等式？（2）如何在没有阈值的情况下学习到目标概念中的(-2)？（3）它的训练样例是怎样的？&lt;/p&gt;

&lt;p&gt;翻了一遍书之后，我认为答案是这样的:&lt;code class=&quot;highlighter-rouge&quot;&gt;（1）不等式代表了一种线性划分，它是二维平面的分类器。如果没有不等式，它就是一条直线，这条直线正是我们要学习的决策面，而决策面只有在不等式的条件下才有存在的意义（没有不等式的划分，哪里会有决策面呢）。（2）虽然线性单元中没有阈值的概念，但实际上观察感知器训练法则和delta法则都会发现他们的o的输出公式中，都包含了&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vec w\cdot \vec x&lt;/script&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;这说明权值&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;是在两种训练方法中都存在的，且&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;都为1，只是由于感知器中存在sgn这样的映射，因此在感知器中&lt;/code&gt;(&lt;script type=&quot;math/tex&quot;&gt;-w_0&lt;/script&gt;)&lt;code class=&quot;highlighter-rouge&quot;&gt;成为了阈值的概念。(3)针对这个目标函数，它的训练样例输出就是 &lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-2+x_1+2x_2&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;，这样最终学习到的权值&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vec w&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;与&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vec x&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;进行点乘就是最后的决策面(&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vec w\cdot \vec x=0&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;)。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;做这个实验的过程中，我体会到了理论研究中很少注意到的一个参数——学习速率，其实在感知器中就已经说明了学习过程的收敛条件之一是&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;要足够小，如果&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;没有足够小，那么权值会不断增大到无穷大。一个基本的技巧是如果你的训练数据中输入的数字都比较大，那么学习速率一定要缩小。例如你的训练数据范围在[-0.1,0.1]，那么你可能需要一个&lt;script type=&quot;math/tex&quot;&gt;10^(-6)&lt;/script&gt;数量级的&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;，如果你的训练数据范围在[-1,1]，那么你需要更小的&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;，你可以监控学习过程中所得到的权值来不断调整这一参数。&lt;/p&gt;

&lt;p&gt;本实验的训练数据使用了[-0.1,0.1]的输入范围，学习速率为0.000005，在E(梯度下降所设定的误差)小于0.1时结束循环。为了快速收敛，没有采用a)问中的&lt;script type=&quot;math/tex&quot;&gt;\eta_0 / i&lt;/script&gt;学习速率(使用这个参数之后，在接近收敛时仍会花费很多时间)。&lt;/p&gt;

&lt;p&gt;学习过程总共经过了136轮权值修正。下图中是前30轮的权值和误差E(可以看出E从开始的急速下降到后面的缓慢下降)。最终的权值为[-1.996601 1.003349 2.002383]。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/ann/perceptron/linear-unit-experiment.png&quot; alt=&quot;实验前30轮权值及误差E&quot; /&gt;&lt;/p&gt;

&lt;p&gt;误差E的前30轮曲线图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/ann/perceptron/errorE.png&quot; alt=&quot;误差E曲线图&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 15 May 2016 08:00:00 +0800</pubDate>
        <link>http://words.hackiey.com/2016/05/15/ann-perceptron-2/</link>
        <guid isPermaLink="true">http://words.hackiey.com/2016/05/15/ann-perceptron-2/</guid>
        
        <category>机器学习</category>
        
        <category>人工神经网络</category>
        
        
      </item>
    
      <item>
        <title>人工神经网络（感知器）</title>
        <description>&lt;p&gt;我曾经一度以为所有的ANN就是由感知器(pecrceptron)组成的，但实际上，感知器只是一种ANN的基础单元。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;感知器&lt;/h3&gt;

&lt;p&gt;感知器以一个实数值向量作为输入，计算这些输入的线性组合，如果结果大于某个&lt;strong&gt;阈值&lt;/strong&gt;，就输出1，否则输出-1。如果输入为&lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt;到&lt;script type=&quot;math/tex&quot;&gt;x_n&lt;/script&gt;，那么感知器计算的输出为:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
o(x_1,...,x_n)=\begin{cases} 
						1,  &amp; \text{if $w_0+w_1x_1+w_2x_2+...+w_nx_n&gt;0$}\\ 
						-1, &amp; \text{otherwise} \\ 
					\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;叫做权值(weight)，是一个&lt;strong&gt;实数常量&lt;/strong&gt;，用来决定输入&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;对感知器输出的贡献率。需要注意的是(&lt;script type=&quot;math/tex&quot;&gt;-w_0&lt;/script&gt;)是一个阈值，如果想要使感知器输出1，其余的&lt;script type=&quot;math/tex&quot;&gt;w_1x_1+...+w_nx_n&lt;/script&gt;必须要大于(&lt;script type=&quot;math/tex&quot;&gt;-w_0&lt;/script&gt;)。从这里可以看出，&lt;code class=&quot;highlighter-rouge&quot;&gt;感知器可以做一个二叉分类器，因为它对所有的输入做出1或-1的输出，是线性分类器(linear classifier)的一种。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/ann/perceptron/perceptron.png&quot; alt=&quot;感知器&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了简化，可以将&lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;设置为1，不等式可写为&lt;script type=&quot;math/tex&quot;&gt;{\sum_{i=0}^nw_ix_i}&gt;0&lt;/script&gt;，这样可以表示为&lt;strong&gt;向量&lt;/strong&gt;&lt;script type=&quot;math/tex&quot;&gt;\vec w\cdot\vec x&gt;0&lt;/script&gt;，因此感知器函数可以写为:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o(\vec x)=sgn(\vec w\cdot\vec x)&lt;/script&gt;

&lt;p&gt;sgn是阈值函数，当x&amp;gt;0时，sgn(x)=1，当x&amp;lt;=0时，sgn(x)=-1。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;感知器的表征能力&lt;/h3&gt;

&lt;p&gt;从上面的几个式子中可以看出来感知器可以做分类，而这个类别只有两种，要么是，要么不是。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我们可以把感知器看作是n维实例空间(即点空间)中的超平面决策面。对于超平面一侧的实例，感知器输出1，对于另一侧输出-1。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;什么是超平面？在N维空间中，任何N-1维的次空间都是超平面，在二维平面中，超平面是一条&lt;strong&gt;直线&lt;/strong&gt;，在三维空间中，超平面就是我们日常所理解的平面。&lt;/p&gt;

&lt;p&gt;不是所有的正反样例集合都可以恰好被分成两份，可以被分割的称为线性可分(linearly separable)样例集合。&lt;/p&gt;

&lt;p&gt;单独的感知器可以表示一些布尔函数。例如在一个有两个输入的感知器中，实现AND函数可以设&lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt;为-0.8，&lt;script type=&quot;math/tex&quot;&gt;w_1=w_2=0.5&lt;/script&gt;，实现OR函数可以设&lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt;为-0.3，&lt;script type=&quot;math/tex&quot;&gt;w_1=w_2=0.5&lt;/script&gt;，还可以实现与非(NAND)和或非(NOR)等函数，使用两层深度的感知器网络就可以实现所有的布尔函数。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;因为阈值单元的网络可以表示大量的函数，而单独的单元不能做到这一点，所以通常我们感兴趣的事学习阈值单元组成的多层网络。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;是一个很重要的概念，它就像y=ax+b中的b，是一个必不可少的常量。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如何训练一个感知器，这里主要考虑两种训练法则，感知器训练法则(perceptron training rule)和delta法则(delta rule)。感知器训练法则在线性可分的训练样例中&lt;strong&gt;保证&lt;/strong&gt;收敛到可接受的假设，delta法则无论是否线性可分都可以收敛，但是在线性不可分时收敛到目标概念的最佳&lt;strong&gt;近似&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;感知器训练法则&lt;/h3&gt;

&lt;p&gt;感知器训练法则的过程是，首先生成随机的权值，然后反复地应用感知器到每一个训练样例，如果感知器分类&lt;strong&gt;错误&lt;/strong&gt;，那么就根据训练样例来修改权值。&lt;strong&gt;重复这个过程直到感知器能够正确分类所有样例&lt;/strong&gt;。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i\leftarrow w_i+\Delta w_i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w_i=\eta (t-o)x_i&lt;/script&gt;

&lt;p&gt;其中t是训练样例的目标输出，o是感知器的输出，&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;是学习速率(learning rate)，它是一个很小的数值，并且它有时会随着权调整次数的增加而衰减(&lt;code class=&quot;highlighter-rouge&quot;&gt;并不是一直不变的&lt;/code&gt;)。&lt;/p&gt;

&lt;p&gt;如果训练样例线性可分，且&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;足够小，那么感知器训练法则可以在&lt;strong&gt;有限次&lt;/strong&gt;内收敛到一个能正确分类所有训练样例的权向量。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;实验&lt;/h3&gt;

&lt;p&gt;这一节的代码实现了AND函数的学习，在多次实验中，发现使用200个以上的训练数据便可以学习到一个完全正确的AND函数。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;有人说神经网络的缺点之一是不理解权值，以及难以调试，在多个单元组成的网络中有时甚至要靠经验去判断使用多少层和多少个基本单元，我同意这样的说法，但对于这样的结果我会更兴奋，因为这就像我正在理解我自己的大脑。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;使用感知器训练法则所学习的AND函数的权值是可以被理解的，这是一张包含了错误与正确的训练结果图，可以发现，正确的权值中|&lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt;|始终大于&lt;script type=&quot;math/tex&quot;&gt;w_1和w_2&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/ann/perceptron/perceptron-result.png&quot; alt=&quot;感知器训练法则结果&quot; /&gt;&lt;/p&gt;

&lt;p&gt;使用感知器训练法则可以解决线性可分的问题，如果训练样例不是线性可分时，感知器训练法则将无法收敛，下一节会介绍适用于样例不是线性可分时的训练方法——delta法则。&lt;/p&gt;
</description>
        <pubDate>Sat, 14 May 2016 08:00:00 +0800</pubDate>
        <link>http://words.hackiey.com/2016/05/14/ann-perceptron-1/</link>
        <guid isPermaLink="true">http://words.hackiey.com/2016/05/14/ann-perceptron-1/</guid>
        
        <category>机器学习</category>
        
        <category>人工神经网络</category>
        
        
      </item>
    
      <item>
        <title>人工神经网络（序）</title>
        <description>&lt;p&gt;ICLR2016的最佳论文是《Neural Programmer-Interpreters》(神经程序解释器),Google DeepMind的Scot Reed和Nando de Freitas提出了一种神经程序解释器，它是一种递归性的合成神经网络，能学习对程序进行表征和执行。(新智元公众号在对这一文章宣传时用了“有望取代初级程序员”，足以说明这篇论文的重要性，在此强烈推荐这个公众号，它每天都会有很多介绍国内外人工智能技术的文章)&lt;/p&gt;

&lt;p&gt;自从接触了人工智能，就一直梦想着可以实现能够写程序的程序，本想从这篇论文中得到一些灵感，但是读完相关工作后就读不懂了，我觉得肯定是因为基础不牢固的原因，NPI的基础是深度学习，深度学习的基础是神经网络，所以我从头开始学习人工神经网络。有人说学习的最好方法是教别人，我越来越赞成这一点了，因为等我足够有名了，肯定会有大神来指出我的错误(doge脸)，不过我一定会努力少犯错误，避免误导初学的小朋友。&lt;/p&gt;

&lt;p&gt;这一系列文章主要参考Tom M. Mitchell的《机器学习》（更像是在抄书的时候加了点自己的理解），除非特别说明，所有的引用均来自这本书。为避免教坏小朋友，我自己理解的部分会使用&lt;code class=&quot;highlighter-rouge&quot;&gt;code标签&lt;/code&gt;包裹起来。&lt;/p&gt;

&lt;h3 id=&quot;ann&quot;&gt;人工神经网络(ANN)&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;技术来源于生活，又高于生活 ——Hackiey&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;飞机的发明已经是老掉牙的故事了，现在我们说说怎么造一个大脑。&lt;/p&gt;

&lt;p&gt;尽管到目前为止，人类还无法理解人类是如何理解东西的，但是生物神经元(neuron)为生物和计算机领域提供了一个学习模型。生物学中，人类的大脑由&lt;script type=&quot;math/tex&quot;&gt;10^{11}&lt;/script&gt;个神经元构成，每个神经元与其他&lt;script type=&quot;math/tex&quot;&gt;10^{4}&lt;/script&gt;个神经元相连，这是一个惊人的数字！因此，即使生物神经元在传递信息时要比计算机慢几乎无数倍，但由于大量的并行计算仍然使得人类可以在极短的时间内做出很多复杂的决策。在计算机向生物学求教时，这几乎是最难迈的坎了(Google的深度学习已经达到数十亿的节点，但仍不能和人脑中的神经网络相比)，现有的计算能力根本无法达到这个要求，不过这并不是什么大问题，虽然摩尔定律最近不怎么灵了，但是早晚有一天我们会造出一个大脑来。&lt;/p&gt;

&lt;p&gt;在具体的介绍之前，要先明确目前ANN的优势和缺陷。ANN适合解决以下特征的问题：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;实例是用很多“属性-值”对表示的&lt;/li&gt;
    &lt;li&gt;目标函数的输出可能是离散值、实数值或者由若干实数属性或离散属性组成的向量&lt;/li&gt;
    &lt;li&gt;训练数据可能包含错误&lt;/li&gt;
    &lt;li&gt;可容忍长时间的训练&lt;/li&gt;
    &lt;li&gt;可能需要快速求出目标函数值&lt;/li&gt;
    &lt;li&gt;人类能否理解学到的目标函数是不重要的&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;第一条中的实例指的是每一个训练实例，在训练过程中，要根据每一个实例的输入和输出来不断调整所学到的目标函数(所学到的权值)，缺一不可。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;ANN的训练过程是缓慢的，但是当训练完成投入使用时，对所学到的网络与输入进行求值时是非常快速的。&lt;/p&gt;

&lt;p&gt;最令人激动的是最后一条，当你训练完一个ANN，你可能无法理解其中的权值代表着什么，但是它们凑在一起却可以获得正确的的答案！正如你无法理解自己的大脑，但是你的大脑却不会欺骗你。&lt;/p&gt;

&lt;p&gt;简介之后进入正文，我会尽量实现每一个介绍到的算法(其实主要是为了练习刚学到的python，如果遇到错误请用力指正)，接下来会按照《机器学习》第四章中的内容逐个介绍ANN的几种主要单元：感知器(perceptron)、线性单元(linear unit)和sigmoid单元(sigmoid unit)。然后介绍训练这些单元组成的多层网络的反向传播算法，并考虑几个一般性问题，比如ANN的 表征能力、假设空间搜索的本质特征、过度拟合问题等。最终会逐步过滤到深度学习，并介绍本文开始提到的NPI(这才是终极目标啊！)。&lt;/p&gt;

&lt;p&gt;实验所涉及的代码可以在&lt;a href=&quot;http://github.com/hackiey/machine-learning&quot;&gt;github&lt;/a&gt;上找到，主要是为了练习一下python，如果时间充裕的话会补上js版和lisp(作为初学者，这个同样是为了做练习)。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;目录&lt;/h3&gt;

&lt;p&gt;1.1 &lt;a href=&quot;/2016/05/14/ann-perceptron-1/&quot;&gt;感知器（一）&lt;/a&gt;(描述了感知器以及感知器训练法则)&lt;/p&gt;

&lt;p&gt;1.2 &lt;a href=&quot;/2016/05/15/ann-perceptron-2/&quot;&gt;感知器（二）（delta法则）&lt;/a&gt;(描述了delta训练法则)&lt;/p&gt;
</description>
        <pubDate>Sat, 14 May 2016 08:00:00 +0800</pubDate>
        <link>http://words.hackiey.com/2016/05/14/ann-introductory/</link>
        <guid isPermaLink="true">http://words.hackiey.com/2016/05/14/ann-introductory/</guid>
        
        <category>机器学习</category>
        
        <category>人工神经网络</category>
        
        
      </item>
    
      <item>
        <title>蜂窝网格最短距离问题</title>
        <description>&lt;p&gt;大概半年前在写蜂窝网格的A*寻路算法时，遇到了如何选择启发式的问题。传统的曼哈顿距离虽然可以正常运行找到正确的最短路径，但是在蜂窝网格地图中，两点间的最短路径不止一条，曼哈顿距离会使路径的选择总是偏向某一方向。根本原因是启发式中的曼哈顿距离并不是两点间真正的最短距离。&lt;/p&gt;

&lt;p&gt;在网上搜索了很多，但是一直没有搜到合适的方法，于是在炎热的某一天，自己动笔寻找这种方法。没想到的是答案竟然十分简单。&lt;/p&gt;

&lt;p&gt;下图采用二维坐标表示了一个蜂窝网格地图，关于二维坐标系的选择，可以参考这一篇文章&lt;a href=&quot;http://www.cnblogs.com/alamiye010/archive/2011/10/17/2214477.html&quot;&gt;《蜂窝拓扑结构在SLG地图布局中的应用》&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/honeycomb-mesh-shortest-path/honeycomb-mesh-2d.png&quot; alt=&quot;蜂窝网格地图二维坐标系&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在二维坐标系中，横坐标为x，纵坐标为y，可以得出结论，p1(x1,y1)和p2(x2,y2)之间的最短距离为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/honeycomb-mesh-shortest-path/formula1.png&quot; alt=&quot;蜂窝网格二维坐标最短距离公式&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注: 公式中的除法是求整数商，比如 3/2=1.&lt;/p&gt;

&lt;p&gt;用这个公式可以计算出(1,1)到(4,4)的最短距离为&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(|1/2-4/2+1-4|+|1-4x|+|(1-4)+(4/2-1/2)+4-1|)/2=5&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;看上去是有一些复杂，所以之前在用二维坐标寻找这个规律时费了很多工夫都没有找到，但是改成使用3个坐标来确定一个点的位置就会方便很多，如下图所示。坐标表示为(x,y,z)，出于对称的美观原因，我将Z轴按照负方向增长。在平面上，只需2个坐标即可表示一个点，所以第三个坐标必然可以通过前两个坐标推导出来，在这幅图中，z=y-x。这其中的数学原因不去深究，总之通过一些计算肯定可以得出其中的关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/honeycomb-mesh-shortest-path/honeycomb-mesh-3d.png&quot; alt=&quot;蜂窝网格地图三维坐标系&quot; /&gt;&lt;/p&gt;

&lt;p&gt;三维坐标中最引人注目的一个性质是，从任意一个点向相邻方向移动1格距离，一定会恰好使两个坐标都发生一次改变，改变的数值均为1。通过这个性质，可以得出两点之间的最短距离为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/honeycomb-mesh-shortest-path/formula2.png&quot; alt=&quot;蜂窝网格三维坐标最短距离公式&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后，将三维坐标改为(x,y,y-x)代入上式，可得出二维坐标的最短距离公式，同时需要对二维坐标进行一些变换以适应蜂窝网格，最终就是第二张图中的公式。&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Jan 2016 20:00:00 +0800</pubDate>
        <link>http://words.hackiey.com/2016/01/21/honeycomb-mesh-shortest-path/</link>
        <guid isPermaLink="true">http://words.hackiey.com/2016/01/21/honeycomb-mesh-shortest-path/</guid>
        
        <category>游戏算法</category>
        
        
      </item>
    
  </channel>
</rss>
