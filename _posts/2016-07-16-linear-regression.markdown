---
layout:     post
title:      "线性回归"
subtitle:   ""
date:       2016-07-16 00:00:00
header-img: img/in-post/ann/ann2.jpg
author:     "Harry"
tags:
    - 机器学习
---

线性回归是机器学习中的基础方法，一个完整的线性回归学习过程可以很好地表达出机器学习的基本思想。这一篇介绍了单变量的线性回归，描述了**代价函数**。

### 数据

> 数据来自Coursera中机器学习课程的作业。

通常来说，在学习过程中要用到两个数据集，一个是训练集，另一个是测试集。数据集的中每个数据样例包括一些输入和输出，学习程序要从训练集中，根据每个训练样例的输入和输出拟合一个假设函数（根据输入计算输出，后面会提到），在得到假设函数之后会将其应用在测试集中观察拟合的结果。通过测试之后，便可根据新的数据输入来预测结果。整个过程就像我们参考历史房价，然后对明年的房价做出预测。通常测试部分大多取决于具体的应用环境，所以这里描述训练集的学习部分。

下方的散点图是不同的人口城市的食品盈利情况，横坐标是不同城市人口数量，纵坐标是快餐车的盈利状况，我们的学习目标就是给一个城市的人口数量，预测快餐车的盈利。

![Scatter plot of training data](/img/in-post/machine-learning/linear-regression/scatter-profit-of-population.png)

### 建立模型

回归问题通常用于两个用途，一是预测得出一个实数输出，输出空间的值是连续的、不间断的；另一个是分类问题，输出得到不同的离散的值，对应于各个不同的分类。快餐车的盈利不是一个分类问题，我们要根据人口情况来预测盈利的值究竟是多少。使用表格来表示数据如下：

| Population of City in 10,000s  (x)| profit in $10,000s  (y)|
| ------------------------------| -------------------:|
| 6.1101                        |17.592               | 
| 5.5277                        |9.1302               |
| 8.5186                        |13.662               |
| ...                           |...                  |

首先根据图表定义一些符号，图表标题栏左侧的

- **m** = 训练样例的个数  	(Number of training examples)
- **x**'s = 输入变量/特征	    ("input" variable/feature)
- **y**'s = 输出变量/目标变量	 ("output" variable/"target" variable)

m是训练样例的个数，在快餐车盈利的数据集中，m为97；x是输入变量，这里指的是城市人口数量，其中$$x^{(i)}$$指的是第i个训练样例中的输入变量；y是输出变量，这里是盈利的金额，其中$$y^{(i)}$$指的是第i个训练样例中的输出变量（带括号的i都不是指数，而是序数）。训练集之所以成为训练集，是因为它把x和y都已列出来，这是“过去的经验”，我们要从这些正确的样例中找出其中的规律，所以学习的目标就是，给定一个x，能预测出与经验相符的y。

学习算法的工作是根据训练集训练出一个假设函数h (hypothesis)，其中h的输入是城市人口，输出是盈利金额。

接下来该如何表示一个假设函数？

在单变量的线性回归(简单线性回归)中，假设函数通常为一条直线：

$$h_\theta(x)=\theta_0+\theta_1 x \tag1$$

其中的$$\theta$$我们起初并不清楚，需要经过下面的训练方法来逐步确定。（为什么公式用没有y？是因为假设函数的目标就是尽量向y靠近而不严格等于y，在拟合度很高情况下，假设函数所得的值可以约等于y，这就可以成为一个学习成绩良好的假设函数）

### 代价函数（cost function）

这部分没有使用快餐车盈利的数据，只在图中画了3个点以便理解。

代价函数又称为损失函数(loss function)，是用来当$$\theta$$取不同的值的时候，评判该取值是否合理的标准，代价函数值越大，$$\theta$$所确定的直线就越偏离训练集，代价函数值越小，就说明这组$$\theta$$所确定的直线越能拟合训练集。为了能使选择的$$\theta1和\theta2$$能够尽量使$$h_\theta(x)$$靠近y（即$$h_\theta(x)-y$$要小），推广到所有的训练样例上，定义代价函数如下：

$$J(\theta_0,\theta_1)={1\over 2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 \tag2$$

关于定义中为什么要有平方和$$1\over 2$$项，主要是为了数学上的方便。而学习算法的工作是选择合适的$$\theta_0和\theta_1$$使得$$J(\theta)$$取得最小值。

下面几幅图从直观上解释了为什么这样的定义可以评判一组$$\theta$$的取值好坏。为了简化，将截距项$$\theta_0$$设为0，观察$$\theta_1$$在取不同值的情况下，$$J(\theta)$$的值如何变化。

| ------------------------------| -------------------:|
| ![cost-function-1](/img/in-post/machine-learning/linear-regression/cost-function-1.jpeg)  | ![cost-function-1](/img/in-post/machine-learning/linear-regression/cost-function-4.png)            | 
| ![cost-function-1](/img/in-post/machine-learning/linear-regression/cost-function-3.png)                        | ![cost-function-1](/img/in-post/machine-learning/linear-regression/cost-function-2.png)              |

第一幅图代表了训练集，现在想要生成一条能够尽量满足这几个点的直线，图2-4分别是$$\theta_1$$取0、0.5和1时的直线，根据公式(2)计算相对应的$$J(\theta)$$为：

$$J(0,0)={1\over 2*3}[(0-1)^2+(0-2)^2+(0-3)^2]={7\over 3}\approx 2.33$$

$$J(0,0.5)={1\over 2*3}[(0.5-1)^2+(1-2)^2+(1.5-3)^2]={7\over 12}\approx 0.58$$

$$J(0,1)={1\over 2*3}[(1-1)^2+(2-2)^2+(3-3)^2]=0$$

直观上可以看出J越小，直线越能拟合所有的点。有了这样的认识之后我们再看一下J的图像，为了简化，仍然把$$\theta_0$$设为1，只作出J关于$$\theta_1$$的图像。

![cost-function-1](/img/in-post/machine-learning/linear-regression/cost-function-5.png)

下一篇文章会介绍如何寻找能够使J最小的$$\theta$$值的算法——**梯度下降**。